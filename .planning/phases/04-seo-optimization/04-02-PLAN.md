---
phase: 04-seo-optimization
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - sitemap.xml
  - robots.txt
autonomous: true

must_haves:
  truths:
    - "sitemap.xml lists all 14 pages with correct absolute URLs"
    - "robots.txt allows all crawlers and references sitemap.xml"
    - "Search engines can discover all public pages via sitemap"
    - "All SEO elements across the site pass automated validation"
  artifacts:
    - path: "sitemap.xml"
      provides: "URL listing for search engine discovery"
      contains: "urlset xmlns"
    - path: "robots.txt"
      provides: "Crawler directives and sitemap reference"
      contains: "Sitemap:"
  key_links:
    - from: "robots.txt"
      to: "sitemap.xml"
      via: "Sitemap: directive"
      pattern: "Sitemap:\\s*https://pattern158\\.solutions/sitemap\\.xml"
    - from: "sitemap.xml"
      to: "all 14 HTML pages"
      via: "loc elements with absolute URLs"
      pattern: "<loc>https://pattern158\\.solutions/"
---

<objective>
Create sitemap.xml and robots.txt for search engine discovery, then validate all SEO elements across the entire site.

Purpose: Enable search engines to discover and index all 14 pages, and verify the complete SEO implementation from Plan 01 + Plan 02 is correct.
Output: sitemap.xml, robots.txt at site root, plus validated SEO across all pages.
</objective>

<execution_context>
@/home/xhiris/.claude/get-shit-done/workflows/execute-plan.md
@/home/xhiris/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-seo-optimization/04-RESEARCH.md
@.planning/phases/04-seo-optimization/04-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create sitemap.xml and robots.txt</name>
  <files>sitemap.xml, robots.txt</files>
  <action>
**Create `sitemap.xml`** at the project root with all 14 page URLs:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://pattern158.solutions/index.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/philosophy.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/faq.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/contact.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/testimonials.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-a.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-b.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-c.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-d.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-e.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-f.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-g.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-h.html</loc>
  </url>
  <url>
    <loc>https://pattern158.solutions/exhibits/exhibit-i.html</loc>
  </url>
</urlset>
```

Per research: Do NOT include `<lastmod>`, `<changefreq>`, or `<priority>` -- Google ignores changefreq/priority, and lastmod is only useful if consistently accurate (we can't guarantee that for a static site with manual updates).

**Create `robots.txt`** at the project root:

```
# Pattern 158 Solutions - Dan Novak Portfolio
# Allow all search engines to crawl all content
User-agent: *
Allow: /

# Sitemap location
Sitemap: https://pattern158.solutions/sitemap.xml
```

Both files MUST be at the project root (same level as index.html).
  </action>
  <verify>
- Verify sitemap.xml exists at project root: `ls -la sitemap.xml`
- Verify robots.txt exists at project root: `ls -la robots.txt`
- Count URLs in sitemap: `grep -c '<loc>' sitemap.xml` -- should be 14
- Verify all URLs are absolute: `grep '<loc>' sitemap.xml | grep -cv 'https://pattern158.solutions/'` -- should be 0
- Verify robots.txt references sitemap: `grep 'Sitemap:' robots.txt` -- should show the sitemap URL
- Verify XML is well-formed: `xmllint --noout sitemap.xml 2>&1` (if xmllint available, otherwise visual inspection)
  </verify>
  <done>sitemap.xml lists all 14 pages with absolute https URLs (no lastmod/changefreq/priority). robots.txt allows all crawlers and references the sitemap.</done>
</task>

<task type="auto">
  <name>Task 2: Validate all SEO elements across the entire site</name>
  <files></files>
  <action>
Run a comprehensive automated validation of all SEO work from Plan 01 and Plan 02. This is a read-only validation task -- no files are modified.

**Validation checks to run (use grep/shell commands):**

1. **Meta description uniqueness**: Extract all meta descriptions from 14 pages, verify all are unique and non-empty. Flag any duplicates.

2. **Meta description length**: Check each description is between 100-170 characters. Flag any too short or too long.

3. **OG tag completeness**: For each of 14 pages, verify presence of: og:type, og:url, og:title, og:description, og:image, og:site_name. Report any missing tags.

4. **Canonical URL correctness**: For each page, verify the canonical href matches the expected absolute URL for that page. Verify og:url matches canonical href.

5. **JSON-LD validity**:
   - Verify index.html has exactly 2 JSON-LD blocks (Person + WebSite)
   - Verify remaining 13 pages each have exactly 1 JSON-LD block (WebSite)
   - Extract JSON-LD content and verify it parses as valid JSON (use python3 -m json.tool or node -e if available)

6. **URL consistency**: Verify all absolute URLs use `https://pattern158.solutions/` domain consistently. No trailing slashes on page URLs. No http:// (must be https://).

7. **Sitemap coverage**: Verify every HTML file in the project has a corresponding `<loc>` entry in sitemap.xml. No extra entries, no missing entries.

8. **robots.txt validation**: Verify Sitemap directive uses correct absolute URL.

9. **No regressions**: Verify existing tags are intact:
   - All pages still have charset, viewport, color-scheme meta tags
   - All pages still have theme detection script
   - All pages still have CSS link
   - All pages still have title tag

**Output format**: Print a summary table of pass/fail for each check category. If any check fails, print the specific failure details.

If all checks pass, print: "SEO VALIDATION COMPLETE: All 14 pages pass all checks."
If any check fails, fix the issue if it is a minor typo/oversight, then re-run validation.
  </action>
  <verify>
All validation checks pass with zero failures. Summary table shows PASS for all categories.
  </verify>
  <done>Comprehensive SEO validation confirms: 14 unique meta descriptions, 14 complete OG tag sets, 14 canonical URLs, correct JSON-LD distribution, 14 sitemap entries, valid robots.txt, and zero regressions to existing functionality.</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. sitemap.xml at root with 14 absolute URLs
2. robots.txt at root with Sitemap directive
3. All SEO validation checks pass (meta, OG, canonical, JSON-LD, sitemap coverage, URL consistency)
4. No changes to existing HTML content or functionality
</verification>

<success_criteria>
- sitemap.xml exists with exactly 14 URL entries using https://pattern158.solutions domain
- robots.txt exists with Allow: / and Sitemap: directive
- Full automated validation passes all checks across all 14 pages
- Phase 4 success criteria met: unique meta descriptions, OG tags for social sharing, JSON-LD for rich results, sitemap for discoverability
</success_criteria>

<output>
After completion, create `.planning/phases/04-seo-optimization/04-02-SUMMARY.md`
</output>
